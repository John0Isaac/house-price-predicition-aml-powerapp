{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J7iXW-XGGN23"
      },
      "source": [
        "# Step 4: Build and Save a Machine Learning Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H3M6Dz0C0ac2"
      },
      "source": [
        "Let's start by making an assumption that a house price in rural area costs 1500 per meter square and a house in the city costs 5000 per meter square from this assumption we will generate some dummy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.11.0\n",
        "!pip install -U ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1688009080201
        },
        "id": "IUiWTOFh0Ye-"
      },
      "outputs": [],
      "source": [
        "# Import needed libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lbz6GQdIG81w"
      },
      "source": [
        "## 4.a Create Normalized Dummy Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1688009085732
        },
        "id": "2ziVtOLI0u0d"
      },
      "outputs": [],
      "source": [
        "# First Column number of bedrooms\n",
        "# Second Column Area in Square meter divided by 100\n",
        "# Third Column Rural or City location 1 represents city and 0 for Rural\n",
        "xs = np.array([['1.0', '1.0', '0.0'],\n",
        "               ['1.0', '0.5', '1.0'],\n",
        "               ['2.0', '2.0', '0.0'],\n",
        "               ['2.0', '1.0', '1.0'],\n",
        "               ['3.0', '3.0', '0.0'],\n",
        "               ['4.0', '4.0', '1.0'],\n",
        "               ['6.0', '6.0', '0.0'],\n",
        "               ['3.0', '3.0', '1.0']\n",
        "             ], dtype=float)\n",
        "# Output label that we are trying to predict divded by 100,000\n",
        "ys = np.array(['1.5', '2.5', '3.0', '5.0', '4.5', '20.0', '9.0', '15.0'], dtype=float)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tkcWeK7NIJeC"
      },
      "source": [
        "## 4.b Create the Machine Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1688009088031
        },
        "id": "ioQMxeCHueHx",
        "outputId": "d6d32d08-0161-4082-a71a-e21da15c151c"
      },
      "outputs": [],
      "source": [
        "# A simple regression model created using tensorflow\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(16, activation='relu', input_shape=[3]),\n",
        "    tf.keras.layers.Dense(1, activation='relu'),\n",
        "    ])\n",
        "# We are using mean squared error loss and stochastic gradient descent optimizer\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='mse',\n",
        "              metrics=['mae', 'mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1688009117759
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Start the training to fit the input data to the target data 1000 times\n",
        "model.fit(xs, ys, epochs=1000)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wfJ4EmKxIDB4"
      },
      "source": [
        "## 4.c Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1688009166671
        },
        "id": "g-ro0cTJ1q7-",
        "outputId": "2c62c580-38ba-41e6-91e3-ad1d795bdb85"
      },
      "outputs": [],
      "source": [
        "# Make a prediction\n",
        "prediction = model.predict([[2.0, 1.0, 1.0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1688009167062
        },
        "id": "ehDRYUsc1uhq",
        "outputId": "728069f1-4478-4c0c-f609-32b7c50fff29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "470000.0\n"
          ]
        }
      ],
      "source": [
        "# Convert the prediction from normalized to real value\n",
        "print(round(round(prediction[0][0], 1)* 100000, 1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0BNT-deTIuZF"
      },
      "source": [
        "**Note: This model is not accurate it may hit 1 time and miss 10 times it's just for illustration.**\n",
        "\n",
        "**You should replace it with your own model**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tif0PmweIUpd"
      },
      "source": [
        "## 4.d Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1688009224215
        },
        "id": "ll-HmD_210RO",
        "outputId": "3d330c34-c529-4706-b452-8df7cc3d1749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /anaconda/envs/azureml_py38/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: ./tf-model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-29 03:26:56.807845: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
          ]
        }
      ],
      "source": [
        "model.save('./tf-model')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vC2Xu0veIdTX"
      },
      "source": [
        "# Step 5: Package the Model using ONNX"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gOvtgXYXJf4f"
      },
      "source": [
        "ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers.\n",
        "\n",
        "You may build your model using tensorflow, pytorch, etc.. but in the end you can use onnx as a unified format for your deployment which makes the deployment process much more easier."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X6exmtVNJ9wd"
      },
      "source": [
        "## 5.a Install needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_lcZTXtxp2T",
        "outputId": "fded3cbe-95e3-4b0c-f69d-c16f8bdf15e0"
      },
      "outputs": [],
      "source": [
        "!pip install onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "rYMHPRKHxwCO",
        "outputId": "91970c8b-0b30-42d0-b4b0-e4797ac9055a"
      },
      "outputs": [],
      "source": [
        "!pip install -U tf2onnx"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BAYcJ5pBKBc4"
      },
      "source": [
        "## 5.b Convert the model to ONNX format"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UUX6R8hcKEjA"
      },
      "source": [
        "If your model is not a tensorflow model no worries.\n",
        "\n",
        "See the guide here ==> https://github.com/onnx/tutorials#converting-to-onnx-format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBuE6Pf9xJYF",
        "outputId": "0c76b03d-70a3-40ff-e4a2-cead1f8e285f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py38/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2023-06-29 03:27:35,170 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
            "2023-06-29 03:27:41,496 - INFO - Signatures found in model: [serving_default].\n",
            "2023-06-29 03:27:41,496 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
            "2023-06-29 03:27:41,496 - INFO - Output names: ['dense_1']\n",
            "2023-06-29 03:27:41,556 - INFO - Using tensorflow=2.2.1, onnx=1.12.0, tf2onnx=1.14.0/8f8d49\n",
            "2023-06-29 03:27:41,556 - INFO - Using opset <onnx, 15>\n",
            "2023-06-29 03:27:41,560 - INFO - Computed 0 values for constant folding\n",
            "2023-06-29 03:27:41,568 - INFO - Optimizing ONNX model\n",
            "2023-06-29 03:27:41,591 - INFO - After optimization: Identity -2 (2->0)\n",
            "2023-06-29 03:27:42,466 - INFO - \n",
            "2023-06-29 03:27:42,466 - INFO - Successfully converted TensorFlow model ./tf-model to ONNX\n",
            "2023-06-29 03:27:42,466 - INFO - Model inputs: ['dense_input']\n",
            "2023-06-29 03:27:42,466 - INFO - Model outputs: ['dense_1']\n",
            "2023-06-29 03:27:42,466 - INFO - ONNX model is saved at ./model/house_price_model.onnx\n"
          ]
        }
      ],
      "source": [
        "!python -m tf2onnx.convert --saved-model ./tf-model --output ./model/house_price_model.onnx"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AFOgKvB7Kqn2"
      },
      "source": [
        "## 5.c Test the ONNX format"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sgkAqgigKut6"
      },
      "source": [
        "Let's confirm that ONNX model produces the same output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1688009276578
        },
        "id": "p7SgkgTkyBbM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import onnxruntime as rt\n",
        "\n",
        "# Load the onnx model\n",
        "onnx_session = rt.InferenceSession('./model/house_price_model.onnx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1688009279760
        },
        "id": "I_Sfcz88zwi0"
      },
      "outputs": [],
      "source": [
        "# Create a sample from the data\n",
        "input_data =  [[[2.0, 1.0, 1.0]]]\n",
        "\n",
        "# Prepare the data in the shape that onnx accepts\n",
        "feed = dict([(input.name, input_data[n]) for n, input in enumerate(onnx_session.get_inputs())]) # {'dense_10_input': [[2.0, 1.0, 1.0]]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1688009280800
        },
        "id": "6iDC1rn42HxD",
        "outputId": "ae47f4d2-c283-4992-f3e2-f478921430e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "470000.0\n"
          ]
        }
      ],
      "source": [
        "# Make a prediction\n",
        "prediction = onnx_session.run(None, feed)[0][0][0]\n",
        "\n",
        "# Convert the prediction from normalized to real value\n",
        "print(round(round(prediction, 1)* 100000, 1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R08crWWwNhdO"
      },
      "source": [
        "# Step 6: Register the Model on Azure ML"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7hTXJpGTOCaE"
      },
      "source": [
        "Before we deploy the model we need 2 files:\n",
        "\n",
        "\n",
        "*   Score.py (needed to load the model and make a prediction when anyone invokes our model endpoint)\n",
        "*   myenv.yml (our environment dependencies which are the libraries needed to run our model)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dXh5QvMJRLo3"
      },
      "source": [
        "## 6.a Create the Environment Dependencies file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QHRE_fDNxc0",
        "outputId": "d1d4fb32-6fdf-4ac4-d0ad-235ba24fbdd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ./model/myenv.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./model/myenv.yml\n",
        "name: inference_environment\n",
        "dependencies:\n",
        "- python=3.8.5\n",
        "- pip\n",
        "- pip:\n",
        "    - azureml-defaults==1.49.0\n",
        "    - pillow==9.2.0\n",
        "    - onnxruntime==1.11.1\n",
        "    - azureml-contrib-services==1.49.0\n",
        "    - numpy==1.21.6"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eMLm5O3IRUUj"
      },
      "source": [
        "## 6.b Create the Scoring file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwJueL7YOwTN",
        "outputId": "80e77edd-e3a0-435e-8d9f-24cee980c2e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./model/score.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./model/score.py\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import numpy as np\n",
        "import onnxruntime as rt\n",
        "\n",
        "from azureml.core.model import Model\n",
        "from azureml.contrib.services.aml_response import AMLResponse\n",
        "\n",
        "\n",
        "\n",
        "def init():\n",
        "    \"\"\"\n",
        "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
        "    You can write the logic here to perform init operations like caching the model in memory\n",
        "    \"\"\"\n",
        "    global session\n",
        "    model_dir = Model.get_model_path(model_name=\"HousePricePrediction-secure\")\n",
        "    # the name of the folder\n",
        "    model_filename = 'house_price_model.onnx'\n",
        "    model_path = os.path.join(model_dir, model_filename)\n",
        "\n",
        "    session = rt.InferenceSession(model_path)\n",
        "    logging.info(\"Init complete\")\n",
        "\n",
        "\n",
        "def run(raw_data):\n",
        "    \"\"\"\n",
        "    This function is called for every invocation of the endpoint to perform the actual scoring/prediction.\n",
        "    \"\"\"\n",
        "    logging.info(\"model 1: request received\")\n",
        "    data = json.loads(raw_data)['input_data']\n",
        "    input_data = np.array(data, dtype=np.float32)\n",
        "    if input_data is not None:\n",
        "        feed = dict([(input.name, input_data[n]) for n, input in enumerate(session.get_inputs())])\n",
        "        prediction = session.run(None, feed)[0]\n",
        "        return AMLResponse(json.dumps({'output_data':prediction.tolist()}), 200)\n",
        "    else:\n",
        "        return AMLResponse(\"bad request\", 400)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aB2sqVBVRXIg"
      },
      "source": [
        "## 6.c Import needed Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1688115680521
        },
        "id": "2sxz480BMS-f"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import requests\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.model import Model\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core import Environment\n",
        "\n",
        "from azureml.core.webservice import AciWebservice, Webservice\n",
        "from azureml.exceptions import WebserviceException"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zrEKZEmjRdJq"
      },
      "source": [
        "## 6.d Initialize Needed Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1688115680793
        },
        "id": "Nrrco1n2MYNU"
      },
      "outputs": [],
      "source": [
        "# The name of the model as it will appear in AzureML\n",
        "aml_model_name = 'HousePricePrediction-secure'  # Updating this requires an update to score.py\n",
        "\n",
        "# The name of the model endpoint to be created in AzureML\n",
        "aci_service_name = 'house-price-prediction-onnx'\n",
        "\n",
        "# The name of the model as it will appear in AI Builder\n",
        "aib_model_name = \"house-price-prediction-v1\"\n",
        "\n",
        "# The local path of the parent of the model directory\n",
        "model_path = '.'\n",
        "\n",
        "is_secure = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1688115708165
        },
        "id": "E8QDfuy7MfCU",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Get your workspace Configuration varibales\n",
        "ws = Workspace.from_config()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VarY0IovR2xr"
      },
      "source": [
        "## 6.e Register the ML Model in Azure ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1688115717713
        },
        "id": "PSfx4N2cMg2s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registering model HousePricePrediction-secure\n",
            "Registered model HousePricePrediction-secure, Version 7\n"
          ]
        }
      ],
      "source": [
        "# Register an AML Model\n",
        "model_root = os.path.join(model_path, './model')\n",
        "model = Model.register(workspace=ws,\n",
        "                       model_path=model_root,\n",
        "                       model_name=aml_model_name,\n",
        "                       tags={'area': \"numbers\", 'type': \"regression\"},\n",
        "                       )\n",
        "\n",
        "print(f\"Registered model {model.name}, Version {model.version}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IH9mGzksR7IU"
      },
      "source": [
        "# Step 7: Deploy the Model to Azure ML"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.a Create an inference configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1688115722159
        },
        "id": "jANNEYpMMi63"
      },
      "outputs": [],
      "source": [
        "entry_script = os.path.join(model_root, \"score.py\")\n",
        "conda_file = os.path.join(model_root, \"myenv.yml\")\n",
        "\n",
        "inference_config = InferenceConfig(runtime=\"python\",\n",
        "                                   entry_script=entry_script,\n",
        "                                   conda_file=conda_file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.b Deploy the the Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1688115849752
        },
        "id": "1lv0MdzTMkW9"
      },
      "outputs": [],
      "source": [
        "service = None\n",
        "\n",
        "try:\n",
        "    # get any existing service with the specified name\n",
        "    service = Webservice(ws, name=aci_service_name)\n",
        "except WebserviceException as e:\n",
        "    print(f\"Webservice not found: {aci_service_name}\")\n",
        "\n",
        "# Update the service with the new model if the service exists, otherwise deploy a new service\n",
        "if service:\n",
        "    print (f\"Updating service {aci_service_name}\")\n",
        "    model = Model(workspace=ws, name=aml_model_name)\n",
        "    service.update(models=[model], inference_config=inference_config, auth_enabled=is_secure)\n",
        "else:\n",
        "    print (f\"Deploying new service {aci_service_name}\")\n",
        "    deployment_config = AciWebservice.deploy_configuration(cpu_cores = 0.8, memory_gb = 1, auth_enabled=is_secure)\n",
        "    service = Model.deploy(ws, aci_service_name, [model], inference_config, deployment_config)\n",
        "\n",
        "service.wait_for_deployment(True)\n",
        "print(service.state)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "52dv2wNxSHJL"
      },
      "source": [
        "## 7.c Test the Deployed Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1688115850959
        },
        "id": "0uPAbbeANEP_"
      },
      "outputs": [],
      "source": [
        "#validate service using response\n",
        "service = Webservice(ws, name=aci_service_name)\n",
        "uri = service.scoring_uri\n",
        "input_data =  [[[2.0, 1.0, 1.0]]]\n",
        "api_key = service.get_keys()[0] # Replace this with the API key for the web service\n",
        "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
        "\n",
        "request_data = json.dumps({'input_data': input_data})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1688115851115
        },
        "id": "P7BGEMmeNVRU"
      },
      "outputs": [],
      "source": [
        "# Send the Request\n",
        "response = requests.post(uri, headers=headers, data=request_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1688115896154
        }
      },
      "outputs": [],
      "source": [
        "print(\"ML API Key: {}\\nML Endpoint: {}\".format(service.get_keys()[0], service.scoring_uri))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
